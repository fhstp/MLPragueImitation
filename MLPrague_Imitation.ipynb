{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q83u0NUhc-I0"
      },
      "source": [
        "# Accelerating AI Through Human Knowledge\n",
        "## Training & Tuning Notebook\n",
        "\n",
        "As part of the workshop you will get the chance to record your own expert trajectories and train your own agents. This notebook accomodates the training and tuning portion. In the first cell bellow you will find two installation commands. Please run this cell within the first 10-15 minutes of the Workshop as this will take some minutes to fully install and can just run in the background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgeEWXdoc-I2"
      },
      "outputs": [],
      "source": [
        "!mkdir recordings agents\n",
        "!pip install swig\n",
        "!pip install git+https://github.com/fhstp/MLPragueImitation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFCNifJIc-I2"
      },
      "source": [
        "This are the imports, you should be able to run these without doing anything else (given the install commands ran without an error)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGZjSK6TSMdD"
      },
      "outputs": [],
      "source": [
        "from imitation_workshop.iqlearn import IQLearn\n",
        "from imitation_gym_wrappers.recorder_wrapper import RecorderWrapper\n",
        "import imitation_workshop.envs\n",
        "import gymnasium as gym\n",
        "import pickle\n",
        "from stable_baselines3 import SAC, PPO\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To show progress in tensorboard, execute the following cell."
      ],
      "metadata": {
        "id": "jqvuacXuw0ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "OVSfBnbbfBFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNvlMKupc-I3"
      },
      "source": [
        "The next cell you see sets the device. It is set to `'cuda'` by default. If you have GPU or TPU runtime available on colab you can leave this as is. Should you only be able to get CPU runtime then you will need to change this to `'cpu'`. The rest of the code wokrs as intended, however training will take longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIQoTBXEc-I4"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3xrdAYvc-I4"
      },
      "source": [
        "## The MountainCar Environment\n",
        "\n",
        "In the following cell you will be able to try your hand at reward shaping. This is setup to be done on the MountainCar Environment (https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/). If you have never seen or worked with the MountainCar environment, it is a rather simple task in which a little cart can accelerate to the left and the right. The task is to build enough momentum to reach the goal atop a steep slope (i.e. the Mountain).\n",
        "\n",
        "In the cell we initiate an instance of the environment and set what type of algorithm we want to use for our agent. In this case we use the IQLearn algorithm to try and solve the task.\n",
        "\n",
        "### Usage\n",
        "\n",
        "In the `step` function there is a variable called `reward` as you can tell by the comment, this is were you can try out your ideas on how the reward can be defined and shaped. To do so, you have the following information at your disposal:\n",
        "+ the cart's previous position\n",
        "+ the cart's previous velocity\n",
        "+ the cart's current position\n",
        "+ the cart's current velocity\n",
        "\n",
        "As the Notebook has the package `numpy` loaded you can implement any ideas on how to define the reward, as long as they can be represented mathematically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44gTsF4gc-I4"
      },
      "outputs": [],
      "source": [
        "class ShapingWrapperMountaincar(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.last_obs = None\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = super().reset(**kwargs)\n",
        "        self.last_obs = obs\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = super().step(action)\n",
        "\n",
        "        last_pos = self.last_obs[0]\n",
        "        last_vel = self.last_obs[1]\n",
        "        pos = obs[0]\n",
        "        vel = obs[1]\n",
        "\n",
        "        # change reward here\n",
        "        reward = -1\n",
        "\n",
        "        self.last_obs = obs\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "env = gym.wrappers.TransformReward(gym.make(\"MountainCarContinuous-v0\"), lambda _: -1)\n",
        "sac = IQLearn(ShapingWrapperMountaincar(env), sac_args={'use_targets': True, 'q_lr': 3e-2, 'policy_lr': 3e-2, 'autotune': True, 'buffer_size': 10000, 'device': device})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncqKUJ2-c-I5"
      },
      "source": [
        "Here we start training the agent. You can also if you wish run the cell more often, as long as you do not rerun the cell above, the agent will just keep training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-lcUTX7c-I6"
      },
      "outputs": [],
      "source": [
        "sac.sac_learn(5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvo-ks5Ec-I6"
      },
      "source": [
        "In the next step we provide a cell that gives a form of evalution. It let's your agent try and solve the task 20 times and gives the average amount of steps it took the agent to reach the goal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8eFprEec-I7"
      },
      "outputs": [],
      "source": [
        "avg_env = gym.make(\"MountainCarContinuous-v0\")\n",
        "\n",
        "steps = np.zeros((20,), dtype=np.uint32)\n",
        "for i in range(20):\n",
        "    obs, info = avg_env.reset()\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    step = 0\n",
        "    while not (terminated or truncated):\n",
        "        action, _states = sac.predict(obs, deterministic=True)\n",
        "        obs, rewards, terminated, truncated, info = avg_env.step(action)\n",
        "        step += 1\n",
        "    steps[i] = step\n",
        "print(f\"{steps.mean()} steps taken on average\")\n",
        "avg_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOMObsnlc-I7"
      },
      "source": [
        "The last cell saves the agent. We provide this as you can now, if you wish to, load your reward shaped agent in the frontend used in this workshop, to see how well your agent solves Mountain Car."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2aiYlWDc-I7"
      },
      "outputs": [],
      "source": [
        "agent_name = 'mountaincar_shaped'\n",
        "save_name = f'agents/{agent_name}_{sac.n_updates}.agent'\n",
        "with open(save_name, 'wb') as f:\n",
        "  pickle.dump(sac, f)\n",
        "print(f'saved agent as {save_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKRXxl7Fc-I8"
      },
      "source": [
        "## Imitation Learning\n",
        "\n",
        "In this part of the Notebook, we start using imitation learning. First again on the example of Mountain Car and later on in the Notebook on a Car Racing environment (https://gymnasium.farama.org/environments/box2d/car_racing/).\n",
        "\n",
        "### Mountain Car imitation\n",
        "\n",
        "At first we define a regularizer, initiate an instance of Mountan Car and define the algorithm to be used as IQLearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF2BNZzIc-I8"
      },
      "outputs": [],
      "source": [
        "def regularizer(x):\n",
        "  return x**2/40\n",
        "\n",
        "env = gym.wrappers.TransformReward(gym.make(\"MountainCarContinuous-v0\"), lambda _: -1)\n",
        "iqlearn = IQLearn(env, regularizer=regularizer, sac_args={'device': device})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJSVbROCc-I8"
      },
      "source": [
        "Next we load the recordings made in the frontend and set this to be used as expert trajectories by our imitator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KYhWuqnc-I8"
      },
      "outputs": [],
      "source": [
        "recording_name = 'recordings/recording'\n",
        "recorder = RecorderWrapper(env, 10000)\n",
        "recorder.load_buffer(recording_name)\n",
        "iqlearn.set_demonstration_buffer(recorder.get_sb3_buffer())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5-r1Pdpc-I8"
      },
      "source": [
        "Here we train the iqlearn agent to be able to use it later on to bias the actual agent towards behaviour displayed by the recordings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxFB22gnc-I9"
      },
      "outputs": [],
      "source": [
        "iqlearn.learn(5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADePLmoWc-I9"
      },
      "source": [
        "Here we again include a way of evaluating the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAaV4Y96c-I9"
      },
      "outputs": [],
      "source": [
        "iqlearn = sac\n",
        "avg_env = gym.make(\"MountainCarContinuous-v0\")\n",
        "\n",
        "steps = np.zeros((20,), dtype=np.uint32)\n",
        "for i in range(20):\n",
        "    obs, info = avg_env.reset()\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    step = 0\n",
        "    while not (terminated or truncated):\n",
        "        action, _states = iqlearn.predict(obs, deterministic=True)\n",
        "        obs, rewards, terminated, truncated, info = avg_env.step(action)\n",
        "        step += 1\n",
        "    steps[i] = step\n",
        "print(f\"{steps.mean()} steps taken on average\")\n",
        "avg_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDJL_IG8c-I9"
      },
      "source": [
        "This cell saves the agent so it can be loaded into the provided frontent to visualize it's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0_7ni8Mc-I-"
      },
      "outputs": [],
      "source": [
        "agent_name = 'mountaincar_imitation'\n",
        "save_name = f'agents/{agent_name}_{iqlearn.n_updates}.agent'\n",
        "with open(save_name, 'wb') as f:\n",
        "  pickle.dump(iqlearn, f)\n",
        "print(f'saved agent as {save_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUeY0Ifec-I-"
      },
      "source": [
        "Now we train the actual agent by using the iq learn agent as a bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3BCmBxkc-I-"
      },
      "outputs": [],
      "source": [
        "sac = IQLearn(env, sac_args={'use_targets': False, 'buffer_size': 10000, 'autotune': False, 'device': device})\n",
        "sac.set_bias_actor(iqlearn.actor)\n",
        "sac.actor.load_state_dict(iqlearn.actor.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIvNrV_Vc-I-"
      },
      "source": [
        "In this cell you can fine tune your agent by adapting the `alpha`value and retraining it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQA9MQDIc-I-"
      },
      "outputs": [],
      "source": [
        "sac.alpha=0.2\n",
        "sac.sac_learn(5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdGeLGHtc-I-"
      },
      "source": [
        "Here you can evaluate the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbLrkI2sc-I-"
      },
      "outputs": [],
      "source": [
        "avg_env = gym.make(\"MountainCarContinuous-v0\")\n",
        "\n",
        "steps = np.zeros((20,), dtype=np.uint32)\n",
        "for i in range(20):\n",
        "    obs, info = avg_env.reset()\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    step = 0\n",
        "    while not (terminated or truncated):\n",
        "        action, _states = sac.predict(obs, deterministic=True)\n",
        "        obs, rewards, terminated, truncated, info = avg_env.step(action)\n",
        "        step += 1\n",
        "    steps[i] = step\n",
        "print(f\"{steps.mean()} steps taken on average\")\n",
        "avg_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGGT31U5c-I-"
      },
      "source": [
        "Just like before in this cell the agent is saved to be visualized in the frontend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQxVFDFbc-I_"
      },
      "outputs": [],
      "source": [
        "agent_name = 'mountaincar_finetuned'\n",
        "save_name = f'agents/{agent_name}_{sac.n_updates}.agent'\n",
        "with open(save_name, 'wb') as f:\n",
        "    pickle.dump(sac, f)\n",
        "print(f'saved agent as {save_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLPcDTdZc-I_"
      },
      "source": [
        "### Car Racing\n",
        "\n",
        "In this section the cells baseically follow the same steps as in Mountain Car before but this time using the Car Racing environment. First a regularizer, an instance of the environment and our biasing agent are defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6ddR57fc-I_"
      },
      "outputs": [],
      "source": [
        "def regularizer(x):\n",
        "  return x**2/40\n",
        "\n",
        "env = gym.make(\"InternalStateCarRacing-v0\")\n",
        "iqlearn = IQLearn(env, regularizer=regularizer, sac_args={'device': device})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPcWvwz5c-I_"
      },
      "source": [
        "Here the recordings saved in the frontend are loaded in and set to be used by die biasing agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gbz3kqr1JLs"
      },
      "outputs": [],
      "source": [
        "recording_name = 'recordings/recording'\n",
        "\n",
        "recorder = RecorderWrapper(env, 10000)\n",
        "recorder.load_buffer(recording_name)\n",
        "iqlearn.set_demonstration_buffer(recorder.get_sb3_buffer())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XlUCGROc-JA"
      },
      "source": [
        "Again we train the biasing agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYib_81EUM98"
      },
      "outputs": [],
      "source": [
        "iqlearn.learn(5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDgp7JYtc-JA"
      },
      "source": [
        "Here the biasing agent is saved so it can be loaded in the frontend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOJnSd_lVlZr"
      },
      "outputs": [],
      "source": [
        "agent_name = 'carracing_imitation'\n",
        "save_name = f'agents/{agent_name}_{iqlearn.n_updates}.agent'\n",
        "with open(save_name, 'wb') as f:\n",
        "  pickle.dump(iqlearn, f)\n",
        "print(f'saved agent as {save_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDOqj3WBc-JA"
      },
      "source": [
        "Should you not want to use the most recently trained biasing agent but rather another version you have, you can use this cell to load your desired biasing agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML2Efo-ee-sh"
      },
      "outputs": [],
      "source": [
        "\n",
        "agent_name = 'carracing_imitation_5000.agent'\n",
        "with open(f'agents/{agent_name}', 'rb') as f:\n",
        "    iqlearn = pickle.load(f)\n",
        "iqlearn.args.device = device\n",
        "iqlearn.actor.to(device)\n",
        "iqlearn.qf1.to(device)\n",
        "iqlearn.qf2.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIIkEtYVc-JB"
      },
      "source": [
        "Here we instantiate a Car Racing environment to give you the opportunity to apply reward shaping on your Car Racing imitaiton agent.\n",
        "\n",
        "#### Usage\n",
        "\n",
        "As before you can use various information to shape your reward. The information is as follows:\n",
        "\n",
        "- offset to the center of the road\n",
        "- the angle of the road in various distances\n",
        "- the car's angular velocity\n",
        "- the car's velocity\n",
        "\n",
        "As before you can use `numpy` to facilitate any calculations you might want to make."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KH0JRPtjc-JB"
      },
      "outputs": [],
      "source": [
        "class ShapingWrapperCarracing(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = super().step(action)\n",
        "\n",
        "        offset = obs[0]\n",
        "        angle_to_road = obs[1]\n",
        "        angle_to_2m = obs[2]\n",
        "        angle_to_5m = obs[3]\n",
        "        angle_to_10m = obs[4]\n",
        "        angular_velocity = obs[5]\n",
        "        velocity = obs[6]\n",
        "\n",
        "        # set reward here\n",
        "        middle_of_lane_reward = 1-np.abs(offset)/10\n",
        "        velocity_reward = 1-(np.abs(velocity-50)/50)\n",
        "        a = 1\n",
        "        b = 1\n",
        "        reward = a*middle_of_lane_reward + b*velocity_reward\n",
        "\n",
        "        if np.abs(offset) > 10:\n",
        "            terminated = True\n",
        "\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "env = gym.make(\"InternalStateCarRacing-v0\")\n",
        "sac = IQLearn(ShapingWrapperCarracing(env), sac_args={'use_targets': True, 'buffer_size': 10000, 'tau': 0.0005, 'autotune': False, 'device': device})\n",
        "sac.set_bias_actor(iqlearn.actor)\n",
        "sac.actor.load_state_dict(iqlearn.actor.state_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyf055Wtc-JB"
      },
      "source": [
        "Here the agent can be fine tuned by adapting `alpha`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4NVGvRGc-JC"
      },
      "outputs": [],
      "source": [
        "sac.alpha=0.5\n",
        "sac.sac_learn(5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEU_reeJc-JC"
      },
      "source": [
        "This cell saves the agent to be visualized in the frontend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8mPf0kwc-JC"
      },
      "outputs": [],
      "source": [
        "agent_name = 'carracing_finetuned'\n",
        "save_name = f'agents/{agent_name}_{sac.n_updates}.agent'\n",
        "with open(save_name, 'wb') as f:\n",
        "    pickle.dump(sac, f)\n",
        "print(f'saved agent as {save_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd2qWkQzc-JC"
      },
      "source": [
        "Here the agent can be evaluated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp-oOzrhc-JD"
      },
      "outputs": [],
      "source": [
        "avg_env = gym.make(\"InternalStateCarRacing-v0\")\n",
        "\n",
        "steps = np.zeros((20,), dtype=np.uint32)\n",
        "np.random.seed(0)\n",
        "for i in range(20):\n",
        "    obs, info = avg_env.reset(seed=np.random.randint(2147483647))\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    step = 0\n",
        "    while not (terminated or truncated):\n",
        "        action, _states = iqlearn.predict(obs, deterministic=True)\n",
        "        obs, rewards, terminated, truncated, info = avg_env.step(action)\n",
        "        step += 1\n",
        "    steps[i] = step\n",
        "print(f\"{steps.mean()} steps taken on average\")\n",
        "avg_env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlpraguews",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}